---
title: 常见机器学习方法
categories: Others
date: 2019-07-09 00:33:42
tags:
---

最近工作需要了解了一下常见的机器学习方法, 顺便水一篇好了...

<!-- 摘要部分 -->
<!-- more -->


- 决策树(Decision Trees): 根据属性构建多层递归二分类路径, 最后将输入对象根据这一系列二分类结果安排在合适的位置
  + 优点: 容易理解和解释
  + 缺点: 缺失数据敏感; 容易过拟合; 不考虑属性的相互关联性(当作独立?)
- 随机森林算法(Random Forest): 在决策树基础上构建. 首先将训练数据分成多个子集, 每个子集生成一个决策树, 然后输入待分类信息给每一个决策树, 最后以少数服从多数的方式决定最后结果
  + 优点: 不容易过拟合, 有很好的抗噪声能力, 对异常点离群点不敏感
  + 缺点: 解释性差, 容易受多分类属性影响
- lasso回归(Lasso regression): 是对一般线性回归的改良, 通过对系数进行惩罚来降低模型复杂度
  + 优点: 解释力强, 解决过拟合
  + 缺点: 惩罚会造成欠拟合
- 逻辑回归(Logistic regression): 使用逻辑回归得到因变量分类结果与输入自变量概率比值的关系
  + 优点: 可解释性强
  + 缺点: 本质是广义线模, 在一些复杂情况下不适用
- 支持向量机(SVM): 将平面上的点以一条直线分开,两类中离直线最近的点的距离最大. 将这个问题拓展到多维空间, 就成了支持向量机
  + 优点: 适用小样本量, 泛用性好
  + 缺点: 可解释性质差, 对缺失数据敏感
- K最近邻算法(KNN): 将新给的数据放在多维空间中, 里面有已知分类的点, 在与其最近的K的点中, 属于某一类的多, 这个点就属于哪一类
  + 优点: KNN理论简单，容易实现
  + 缺点: 计算量大, 每次分类都重新计算, 受分类不平衡影响大
- 朴素贝叶斯(Naive Bayes): 是用贝叶斯公式计算处于某类内的概率
  + 优点: 实现简单与只需要少量的训练数据, 大量计算时速度快
  + 缺点: 应用朴素贝叶斯分类器时必须满足条件: 所有的属性都是条件独立的
- K均值算法(K-Mean): 聚类算法, 无监督学习; 首先在一堆数据中选K个初始点, 计算所有点与这K个点的距离, 然后将点与K个中最近的那一个归成一类. 在第一轮分类完成后, 根据每一类的点计算一个平均的中心点(K个), 然后再将所有点到这K个中心点的距离计算以便, 并将点归到最近的一类. 如此重复下去. 当重复一定次数后, 分组会固定不动, 此时停止.
  + 优点: 异常值不敏感
  + 缺点: 数据量大时计算量大, 数据量少则效果不好; 易受分类不平衡影响
- Adaboost算法: 将多个分类器(多种模型)组合起来, 达到更好的分类效果.
  + 优点: 结合多种基本算法, 考虑不同分类器的权重
  + 缺点: 训练耗时
- 神经网络(NN): 比较复杂的深度学习(DL)方法, 个人理解神经网络中的神经元类似决策树, 会根据输入数据得到一个分类结果, 然后将这个结果交给下游神经元, 下游神经元接受上游信息后得出结果继续传递, 最后将最后一层的结果汇总起来给出分类结果.
  + 优点: 分类准确度高, 鲁棒性和容错性较强
  + 缺点: 深度学习, 计算量大, 解释性差
- 马尔可夫链(MarCov Model): 没有看懂...


主要内容摘自:
[轻松看懂机器学习十大常用算法](https://cloud.tencent.com/developer/article/1006091)
[机器学习算法优缺点及其应用领域](https://blog.csdn.net/mach_learn/article/details/39501849)

当然...我写的解释完全不如这个视频讲的简单清楚明白, 果然现在学习要上B站了么...
<iframe src="//player.bilibili.com/player.html?aid=20922906&cid=34291885&page=1" width="780" height="480" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
[视频出处](https://www.bilibili.com/video/av20922906?t=585), 喜欢的话一定去关注一下up然后三连一波哟~