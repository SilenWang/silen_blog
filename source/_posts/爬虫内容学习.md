---
title: 爬虫内容学习
categories: Script
date: 2018-09-08 17:44:21
tags: ['python', '爬虫']
---

之前没有系统对爬虫相关的东西学习过, 边看[视频](https://www.bilibili.com/video/av18202461/?p=7)边做个笔记.

<!-- more -->

# request / response

request是浏览器/程序向服务器发出的一些信息, 用于请求需要展示的内容, resoponse则是服务器对接收到的请求给出的回应信息.

- request的内容信息
    + 请求方式: GET / POST(最常用)
        * POST相对GET多了form data
        * GET的参数直接包含在URL中, 而POST请求则包含在表单中
    + 请求URL: URL是统一资源定位符, 就是文件/对象的链接
    + 请求头: 重要的配置信息, 以键值对方式存储
    + 请求体: GET时一般无信息, POST需要

- response的内容信息
    + 响应状态码: 用来表示请求情况的数字代码
    + 响应头
    + 相应体: 请求的结果

# python request模块使用

requests基于urllb3, 使用更方便功能更丰富

## GET请求相关

### 请求参数添加

- params参数可以方便的给请求增加参数, 省去手动编写URL

```python
data = {
    'arg1': '1',
    'arg2': '2'
}
response = request.get('url', params=data)
```

### json解析

- 提供了`json`方法, 可直接把返回的json字符串变成json对象

```python
response = request.get('url', params=data)
response.json()
```

### 二进制数据获取

- 使用get请求直接请求图片即可, 然后写入的时候以'wb'形式写入文件即可

### 添加header

- 主要是为了保证获取成功, 有些网站会识别User-Agent以防止机器爬取

```python
headers = {
    'User-Agent': 'XXXX'
}
response = requests.get('url', headers = headers)
```

## POST请求相关

### 请求参数 / 头添加

- 同GET部分(见上)

### response属性

- 常用属性包括:
    + status_code
    + headers
    + cookies
    + url
    + history

### 状态码分析

- requests库本身内置了状态码的分类情况, 所以直接调用内置的信息就可以快速判断请求是否正常/成功, 比如`response.status_code.ok`就相当于`200`

```python
response = requests.get('url')
exit() if response.status_code != response.status_code.ok else print('All Right')
```

### 代理设置

```python
proxy = {
    'http': 'http://127.0.0.1:9743'
    'https': 'https://user:passwd@127.0.0.1:9743'
}

response = requests.get('url', proxies=proxy)
```

如果要使用ss, 则需要另外安装插件

```bash
pip install 'requests[socks]'
```

```python
proxy = {
    'socks5': 'http://127.0.0.1:9743'
}

response = requests.get('url', proxies=proxy)
```

### 超时设置

可以结合`try`进行异常处理

```python
response = requests.get('url', timeout = 1)
```


# 杂项

- 一般第一个get请求得到的是网页的框架, 然后解析框架时再发出新的请求把需要的内容填充到页面
- 如果需要获得相应信息需要分析ajax请求
    - Selenium/WebDriver
    - Splash